[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cleaning transcriptomic datasets",
    "section": "",
    "text": "Preface\nIn this book I will show how several publicly available datasets are filtered and organized in a way that is easy to do transcriptomic analysis with them.\nThe book is divided in several chapters, one for each dataset."
  },
  {
    "objectID": "tcga.html",
    "href": "tcga.html",
    "title": "1  TCGA",
    "section": "",
    "text": "Get dataset from tcga following procedure in https://www.bioconductor.org/packages/release/workflows/vignettes/SingscoreAMLMutations/inst/doc/workflow_transcriptional_mut_sig.html#3_Downloading_and_preparing_the_data\n\nlibrary(dplyr)\nlibrary(TCGAbiolinks)\nlibrary(SummarizedExperiment)\nlibrary(DESeq2)\nlibrary(edgeR)\nlibrary(rtracklayer)\n\n\nsetwd(file.path(\"~\", \"Documents\", \"BioRepo\"))\n\nquery_rna &lt;- TCGAbiolinks::GDCquery(\n    project = 'TCGA-BRCA',\n    data.category = 'Transcriptome Profiling',\n    data.type = 'Gene Expression Quantification',\n    workflow.type = 'HTSeq - Counts'\n)\n\nrnaseq_res &lt;- TCGAbiolinks::getResults(query_rna)\n# the number of rows corresponds to the number of rna-seq samples. notice\n# that you can have two samples coming from the same patient.\ndim(rnaseq_res)\n\nPath to download data:\n\ntmp_dir &lt;- tempdir()\ndatapath &lt;- file.path(tmp_dir, 'GDCdata')\nTCGAbiolinks::GDCdownload(query_rna, directory = datapath)\nbrca_se &lt;- TCGAbiolinks::GDCprepare(query_rna, directory = datapath)\n\nWe will save the steps in the meantime in case there is a problem in the pipeline, therefore we don’t redo all the steps\n\n# save brca_se containing all samples\nsaveRDS(\n    brca_se, \n    file.path(\"Data\", \"20210313_tcga_brca\", \"tcga_brca_all.rds\")\n)\n\n\nbrca_se &lt;- readRDS(\"Data/20210313_tcga_brca/tcga_brca_all.rds\")\n# remove samples that we are not interested. so we only get tumor samples\n# from patients with clinical data. we can get the same patients as \n# from the clinical_data dataframe\nclinical_data_gdc &lt;- colData(brca_se)\nclinical_data_gdc &lt;- clinical_data_gdc[!clinical_data_gdc$is_ffpe, ]\nclinical_data_gdc &lt;- clinical_data_gdc[clinical_data_gdc$sample_type_id == \"01\", ]\n\nWe now filter the clinical data to remove those patient in stage x as these are tumors that couldn’t be evaluated\n\nclinical_data_gdc &lt;-\n    clinical_data_gdc[!is.na(clinical_data_gdc$tumor_stage), ]\nclinical_data_gdc &lt;-\n    clinical_data_gdc[clinical_data_gdc$tumor_stage != \"stage x\", ]\n\n# get molecular subtypes and append to clinical data\nsubtypes &lt;- TCGAbiolinks::PanCancerAtlas_subtypes() %&gt;%\n    dplyr::filter(cancer.type == \"BRCA\") %&gt;%\n    dplyr::filter(substring(pan.samplesID, 14, 15) == \"01\") %&gt;%\n    dplyr::filter(!duplicated(pan.samplesID)) %&gt;% data.frame()\n\nrownames(subtypes) &lt;- subtypes$pan.samplesID\n\nclinical_data_gdc$molecular_subtype &lt;- \n    subtypes[rownames(clinical_data_gdc), \"Subtype_mRNA\"]\n\n# remove patients without molecular subtype data\nclinical_data_gdc &lt;- \n    clinical_data_gdc[!is.na(clinical_data_gdc$molecular_subtype), ]\n\nWe now calculate the time to death or last followup and append to the clinical data. this is the time to event we are analysing.\n\ntime_status_patients &lt;-\n    apply(\n        clinical_data_gdc,\n        1,\n        function(x){\n            \n            days_to_death &lt;- x[[\"days_to_death\"]]\n            days_to_last_follow_up &lt;- x[[\"days_to_last_follow_up\"]]\n            status_patient &lt;- c()\n            \n            if(is.na(days_to_death) && is.na(days_to_last_follow_up)){\n                time_patient &lt;- NA\n                status_patient &lt;- c(status_patient, NA)\n            } else if (is.na(days_to_death)) {\n                time_patient &lt;- as.numeric(days_to_last_follow_up)\n                status_patient &lt;- c(status_patient, 1)\n            } else {\n                time_patient &lt;- as.numeric(days_to_death)\n                status_patient &lt;- c(status_patient, 2)\n            }\n            \n            c(time_patient, status_patient)\n        }\n    )\n\nclinical_data_gdc$status &lt;- time_status_patients[2, ]\nclinical_data_gdc$time &lt;- time_status_patients[1, ]\n\n# remove the patients with no time or status \nclinical_data_gdc &lt;- clinical_data_gdc[\n    (!is.na(clinical_data_gdc$time)) & (!is.na(clinical_data_gdc$status)),\n    \n]\n\nclinical_data_gdc &lt;- clinical_data_gdc[\n    clinical_data_gdc$time != 0,\n    \n]\n\nLet us add the ER and PR status for the patients. For this we need to use another dataset that was downloaded from GDC.\n\nclinical_data_er &lt;- \n    read.csv(\n        file.path(\n            \"Data\",\n            \"20210313_tcga_brca\",\n            \"clinical_data\",\n            \"nationwidechildrens.org_clinical_patient_brca.txt\"\n        ), \n        header = 1, \n        sep = \"\\t\"\n    )\n\nrownames(clinical_data_er) &lt;- clinical_data_er$bcr_patient_barcode\n\n# add er and pr status to original clinical data\nclinical_data_gdc$er_status &lt;- \n    clinical_data_er[\n        clinical_data_gdc[, \"patient\"],\n        \"breast_carcinoma_estrogen_receptor_status\"\n    ]\n\nclinical_data_gdc$pr_status &lt;- \n    clinical_data_er[\n        clinical_data_gdc[, \"patient\"], \n        \"breast_carcinoma_progesterone_receptor_status\"\n    ]\n\n# keep only the patients filtered above\nbrca_se &lt;- brca_se[, rownames(clinical_data_gdc)]\ncolData(brca_se) &lt;- clinical_data_gdc\n\nsaveRDS(brca_se, file.path(tmp_dir, \"brca_se_filter.Rdata\"))\n\n\n# filter genes lowly expressed in 30% of the samples\nbrca_dge &lt;- edgeR::DGEList(counts = assay(brca_se), genes = rowData(brca_se))\nprop_expressed &lt;- rowMeans(cpm(brca_dge) &gt; 1)\nkeep &lt;- prop_expressed &gt; 0.3\n\n# check the distribution of the counts\nop = par(no.readonly = TRUE)\npar(mfrow = c(1, 2))\nhist(cpm(brca_dge, log = TRUE), main = 'Unfiltered', xlab = 'logCPM')\nabline(v = log(1), lty = 2, col = 2)\nhist(cpm(brca_dge[keep, ], log = TRUE), main = 'Filtered', xlab = 'logCPM')\nabline(v = log(1), lty = 2, col = 2)\n\n\nbrca_dge &lt;- brca_dge[keep, , keep.lib.sizes = FALSE]\nbrca_se &lt;- brca_se[keep, ]\n\nsaveRDS(\n    brca_se, \n    file.path(tmp_dir, \"brca_se_filter_gene.Rdata\")\n)\n\nNow we get the gene length information for the normalization step.\n\ngencode_file &lt;- 'gencode.v22.annotation.gtf.gz'\ngencode_link &lt;- \n    paste(\n        'ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_22',\n        gencode_file,\n        sep = '/'\n    )\n\npath_to_gencode &lt;- file.path(\"Data\", \"20210313_tcga_brca\", gencode_file)\n\ndownload.file(gencode_link, path_to_gencode, method = 'libcurl')\n\ngtf &lt;- rtracklayer::import.gff(\n    path_to_gencode, \n    format = 'gtf', \n    genome = 'GRCm38.71', \n    feature.type = 'exon'\n)\n\n# split records by gene to group exons of the same gene\ngrl &lt;- reduce(split(gtf, elementMetadata(gtf)$gene_id))\ngene_lengths &lt;-\n    sapply(\n        grl, \n        function(x) {\n            #sum up the length of individual exons\n            c('gene_length' = sum(width(x)))\n        }\n    )\n\ngene_lengths_tmp &lt;- \n    data.frame(gene_length = gene_lengths, row.names = names(grl))\n\n#extract information on gene biotype\ngenetype &lt;- unique(elementMetadata(gtf)[, c('gene_id', 'gene_type')])\ncolnames(genetype)[1] &lt;- 'ensembl_gene_id'\nrownames(genetype) &lt;- genetype$ensembl_gene_id\n\ngene_lengths_tmp$gene_type &lt;- \n    genetype[rownames(gene_lengths_tmp), \"gene_type\"] \ngene_lengths_tmp$ensembl_gene_id &lt;- \n    genetype[rownames(gene_lengths_tmp), \"ensembl_gene_id\"] \n\n#remove ENSEMBL ID version numbers\ngene_lengths_tmp$ensembl_gene_id &lt;- \n    gsub('\\\\.[0-9]*', '', gene_lengths_tmp$ensembl_gene_id)\n\nsaveRDS(\n    gene_lengths_tmp, \n    file = file.path(tmp_dir, \"gene_lengths_HTSeq_gencodev22.rds\")\n)\n\ngene_lengths &lt;- gene_lengths_tmp\nrownames(gene_lengths) &lt;- gene_lengths$ensembl_gene_id\nrowData(brca_se)$gene_length &lt;- gene_lengths[rownames(brca_se), 'gene_length']\nrowData(brca_se)$gene_biotype &lt;- gene_lengths[rownames(brca_se), 'gene_type']\n\n#annotate gene lengths for the DGE object\nbrca_dge$genes$length &lt;- gene_lengths[rownames(brca_dge), 'gene_length']\n\n# calculate the tmm normalization from edgeR\nbrca_dge_tmm &lt;- calcNormFactors(brca_dge, method = 'TMM')\n\n# compute log FPKM values and append to assays\nassay(brca_se, 'logFPKM_TMM') &lt;- rpkm(brca_dge_tmm, log = TRUE)\nassay(brca_se, \"vst\") &lt;- DESeq2::vst(assay(brca_se))\n\nSave the final dataset.\n\nsaveRDS(\n    brca_se, \n    file = file.path(\"Data\", \"20210313_tcga_brca\", \"tcga_brca_tumor_filtered.rds\")\n)"
  },
  {
    "objectID": "metabric.html#packages-to-use",
    "href": "metabric.html#packages-to-use",
    "title": "2  METABRIC",
    "section": "2.1 Packages to use",
    "text": "2.1 Packages to use\nFirst load the packages that will be used along the cleaning process. Always do this at the beginning of your script to make things organized.\n\nlibrary(DESeq2)\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nlibrary(dplyr)"
  },
  {
    "objectID": "metabric.html#introduction",
    "href": "metabric.html#introduction",
    "title": "2  METABRIC",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nTo download the clinical data and expression levels from the METABRIC cohort go to cbioportal and select the respective cohort (METABRIC): https://www.cbioportal.org/\nHere we will load and format the metabric data in the same way as SCAN-B and the AI dataset, so it is standardized and better to use in future analysis. For this, we use the summarized experiment object to store expression data and clinical information."
  },
  {
    "objectID": "metabric.html#downloading-data",
    "href": "metabric.html#downloading-data",
    "title": "2  METABRIC",
    "section": "2.3 Downloading data",
    "text": "2.3 Downloading data\nLoading gene expression levels.\n\nexpression_data &lt;- read.csv(\n    \"data_mrna_agilent_microarray.txt\",\n    sep = \"\\t\",\n    check.names = FALSE\n) %&gt;% dplyr::mutate(Entrez_Gene_Id = NULL) %&gt;%\n    `colnames&lt;-`(stringr::str_replace_all(colnames(.), stringr::fixed(\".\"), \"-\"))\n\ndim(expression_data)\n\nWe note that the first two columns correspond to HUGO symbol and ENTREZ ID.\nLoad now clinical data.\n\nclinical_data &lt;- read.csv(\n    \"data_clinical_patient.txt\",\n    sep = \"\\t\",\n    comment.char = \"#\"\n)\n\nglimpse(clinical_data)"
  },
  {
    "objectID": "metabric.html#cleaning-and-integrating",
    "href": "metabric.html#cleaning-and-integrating",
    "title": "2  METABRIC",
    "section": "2.4 Cleaning and integrating",
    "text": "2.4 Cleaning and integrating\nWe see that there are over 2500 rows, meaning that we have more patients with clinical data than expression levels. Let us now select patients that have expression levels.\n\nlength(unique(clinical_data$PATIENT_ID)) == nrow(clinical_data)\n\nWe see that each row has a unique identifier, the patient ID, so we set this as a rowname.\n\nrownames(clinical_data) &lt;- clinical_data$PATIENT_ID\n\nAnd we check if all patients from expression data have clinical data.\n\nlength(\n    intersect(\n        clinical_data$PATIENT_ID, \n        colnames(expression_data[, -c(1,2)])\n    )\n) == ncol(expression_data)-2\n\nIndeed it has, therefore we can just subselect clinical data.\n\nduplicated_genes &lt;- expression_data$Hugo_Symbol[\n    duplicated(expression_data$Hugo_Symbol)\n]\n\nhugo_symbols_duplicated &lt;- expression_data$Hugo_Symbol[\n    duplicated(expression_data$Hugo_Symbol)\n] %&gt;% unique\n\nmedian_genes &lt;- sapply(\n    hugo_symbols_duplicated,\n    function(gene, df){\n        df %&gt;% dplyr::filter(Hugo_Symbol == gene) %&gt;%\n            dplyr::mutate(Hugo_Symbol = NULL) %&gt;%\n            as.matrix(.) %&gt;%\n            MatrixGenerics::colMedians(.)\n    },\n    df = expression_data\n)\n\nmetabric_exp &lt;- expression_data[!duplicated(expression_data$Hugo_Symbol), ]\nrownames(metabric_exp) &lt;- expression_data$Hugo_Symbol[\n    !duplicated(expression_data$Hugo_Symbol)\n]\nmetabric_exp$Hugo_Symbol &lt;- NULL\nmetabric_exp[colnames(median_genes), ] &lt;- median_genes %&gt;% t\n\n\nclinical_data &lt;- clinical_data[\n    intersect(colnames(metabric_exp), rownames(clinical_data)), \n]\n\nBefore we just average the duplicated genes median intensity.\n\n# add new clinical data to the summarized experiment object.\nmetabric &lt;- SummarizedExperiment::SummarizedExperiment(\n    assays = list(\n        median_intensity = metabric_exp[, rownames(clinical_data)]\n    ), \n    colData = clinical_data\n)\n\nAnd we can finally save the RDS file to load it up faster later.\n\n# save the final dataset\nsaveRDS(\n    metabric, \n    file = file.path(\"metabric_filtered.rds\")\n)"
  },
  {
    "objectID": "scanb_new.html#loading-rdata-files",
    "href": "scanb_new.html#loading-rdata-files",
    "title": "3  SCANB (2022 release)",
    "section": "3.1 Loading Rdata files",
    "text": "3.1 Loading Rdata files\nFirst we load the Rdata files that contain the gene expression levels and also the gene annotation.\n\nsapply(\n    list.files(\n        path = \"../../Data/20230125_scanb\", \n        full.names = TRUE, \n        pattern = \"Rdata\"\n    ),\n    load,\n    envir = globalenv()\n)\n\nscanb_9206 &lt;- SCANB.9206.mymatrix\nrm(SCANB.9206.mymatrix)\n\nabim_100 &lt;- ABiM.100.mymatrix\nabim_405 &lt;- ABiM.405.mymatrix\nrm(ABiM.100.mymatrix)\nrm(ABiM.405.mymatrix)\n\nnormal_66 &lt;- Normal.66.mymatrix\nrm(Normal.66.mymatrix)\n\noslo_103 &lt;- OSLO2EMIT0.103.mymatrix\nrm(OSLO2EMIT0.103.mymatrix)\n\ngene_id_ann &lt;- Gene.ID.ann\nrm(Gene.ID.ann)\n\ndatasets &lt;- list()\nwhich_assay &lt;- list()\n\nAll the gene anottations are available in the object gene_id_ann, which will be used for scoring."
  },
  {
    "objectID": "scanb_new.html#loading-clinical-data-from-scan-b",
    "href": "scanb_new.html#loading-clinical-data-from-scan-b",
    "title": "3  SCANB (2022 release)",
    "section": "3.2 Loading clinical data from SCAN-B",
    "text": "3.2 Loading clinical data from SCAN-B\n\nsheets &lt;- c(\"SCANB.9206\", \"ABiM.100\", \"OSLO2EMITO.103\", \"ABiM.405\", \"Normal.66\")\n\nclin_data &lt;- suppressWarnings({readxl::read_excel(\n    \"../../Data/20230125_scanb/Supplementary Data Table 1 - 2023-01-13.xlsx\",\n    sheet = sheets[1], \n    progress = TRUE\n)})\n\ndatasets$scanb &lt;- SummarizedExperiment::SummarizedExperiment(\n    assays = list(\"FPKM\" = scanb_9206[, dplyr::pull(clin_data, \"GEX.assay\")]),\n    colData = clin_data %&gt;% data.frame %&gt;%\n        `rownames&lt;-`(clin_data$GEX.assay)\n)\nwhich_assay$scanb &lt;- \"FPKM\"\n\n\n# convert ensembl genes to hugo IDs\nrownames(datasets$scanb) &lt;- gene_id_ann[rownames(scanb_9206), \"Gene.Name\"]\n\n# check what are the duplicated genes to see if we can safely drop them\ndup_genes &lt;- rownames(datasets$scanb)[duplicated(rownames(datasets$scanb))] %&gt;%\n    table\n\ndup_genes\n\nDue to the lower amount of genes and the fact each gene has only one copy duplicated, we select the first one.\n\ndatasets$scanb &lt;- datasets$scanb[!duplicated(rownames(datasets$scanb)), ]\n\nsaveRDS(datasets$scanb, \"../../Data/20230125_scanb/scanb_sumexp.rds\")"
  },
  {
    "objectID": "ai.html#packages-to-use",
    "href": "ai.html#packages-to-use",
    "title": "4  AI - GSE105777",
    "section": "4.1 Packages to use",
    "text": "4.1 Packages to use\nFirst load the packages that will be used along the cleaning process. Always do this at the beginning of your script to make things organized.\n\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\n\nlibrary(readxl)\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "ai.html#introduction",
    "href": "ai.html#introduction",
    "title": "4  AI - GSE105777",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nWe will download and process the aromatase inhibitor data available at GEO under accession code: GSE105777. The data is associated to the paper https://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-019-1223-z.\nKi67 levels are also available to these patients in the supplementary data. We will also download the data available there and combine it here."
  },
  {
    "objectID": "ai.html#downloading-data",
    "href": "ai.html#downloading-data",
    "title": "4  AI - GSE105777",
    "section": "4.3 Downloading data",
    "text": "4.3 Downloading data\nTo download the data use the package GEOquery from Bioconductor and specify the accession code. Save also the data on a directory of your choice. In this case I chose a subfolder called data in my current directory.\n\ngse_ai &lt;- GEOquery::getGEO(\n    \"GSE105777\", \n    destdir = \"./data\", \n    GSEMatrix = TRUE\n)[[1]]\n\nThe function exprs from the package Biobase (automatically loaded if you have bioconductor) let us check the expression levels of the expression set downloaded with GEOquery.\n\nexprs(gse_ai)[, 1:5] %&gt;% head\n\nAnd since this is a microarray experiment we need information to map the probe ids to the genes. This information is stored in the feature slot of the expression set above. To retrieve it use the function fData.\n\nfData(gse_ai) %&gt;% head\n\nAnd now load the ki67 levels data from the supplementary material. Remember to specify the folder where your file is located. The table is pivoted for the ki67 levels, such that each patient will have two rows, one for baseline and another for surgery.\n\nki67_levels &lt;- readxl::read_excel(\n    path = \"data/13058_2019_1223_MOESM2_ESM.xlsx\",\n    range = \"TableS4!A3:L257\", \n) %&gt;% janitor::clean_names() %&gt;% \n    tidyr::pivot_longer(\n        cols = c(\"baseline_ki67\", \"surgery_ki67\"), \n        names_to = \"timepoint\",\n        values_to = \"ki67\"\n    ) %&gt;% \n    dplyr::mutate(ki67 = as.numeric(ki67)) %&gt;%\n    dplyr::mutate(\n        title = ifelse(\n            group == \"Peri AI\",\n            paste0(\n                \"AI.\", \n                number_254_tumours_control_56_ai_treated_198,\n                ifelse(timepoint == \"baseline_ki67\", \"B\", \"S\")\n            ),\n            paste0(\n                number_254_tumours_control_56_ai_treated_198,\n                ifelse(timepoint == \"baseline_ki67\", \"B\", \"S\")\n            )\n        )\n    )\n\nki67_levels %&gt;% head"
  },
  {
    "objectID": "ai.html#cleaning-and-integrating",
    "href": "ai.html#cleaning-and-integrating",
    "title": "4  AI - GSE105777",
    "section": "4.4 Cleaning and integrating",
    "text": "4.4 Cleaning and integrating\nWe start now integrating the clinical data available from the GEOquery with the ki67 levels data.\n\n# first convert the title of control samples (reanalysis) to the same\n# format as the treated patients. this will make it easier down the line\n# to match samples\ncol_data &lt;- pData(gse_ai) %&gt;%\n    dplyr::mutate(\n        title = ifelse(\n            !stringr::str_detect(title, \"reanalysis\"), \n            title,\n            paste0(\n                \"Control.\", \n                as.integer(stringr::str_extract(title, \"\\\\d+\")),\n                ifelse(`sampling time:ch1` == \"diagnosis\", \"B\", \"S\")\n            )\n        )\n    )\n\ncol_data$title %&gt;% tail\n\nWe now add both together. The function inner_join from the dplyr package is very powerful. Remember to always specify by which column you want to merge. It is usually very fast to merge tables this way, since it handles the matching of the columns for you.\n\nfinal_col_data &lt;- dplyr::inner_join(\n    ki67_levels,\n    col_data %&gt;% tibble::rownames_to_column(var = \"gsm_name\"),\n    by = \"title\"\n)\n\nTo check the total number of patients, use the code below.\n\nfinal_col_data$patient_id %&gt;% table %&gt;% length\n\nThe number of controls are:\n\nfinal_col_data %&gt;% dplyr::filter(\n    group == \"No Peri AI\"\n) %&gt;% janitor::tabyl(number) %&gt;% nrow\n\nAnd the number of treated patients are:\n\nfinal_col_data %&gt;% dplyr::filter(\n    group == \"Peri AI\"\n) %&gt;% janitor::tabyl(number) %&gt;% nrow\n\nAnd since we have multiple illumina IDs mapping to the same gene, we take their average.\n\n# we first start removing the duplicated genes, but\n# we will change the average expression levels later\ngse_ai &lt;- gse_ai[-which(is.na(exprs(gse_ai)), arr.ind = TRUE)[, 1], ]\n\nBelow we calculate the average intensity for probe ids with multiple hugo symbols.\n\n# first get the duplicated symbols\nduplicated_symbols &lt;- fData(gse_ai) %&gt;%\n    janitor::tabyl(ILMN_Gene) %&gt;%\n    dplyr::filter(n &gt; 1)\n\n# and check what are the probe ids available in the data\nduplicated_ilmns &lt;- fData(gse_ai) %&gt;% dplyr::filter(\n    ILMN_Gene %in% duplicated_symbols$ILMN_Gene\n)\n\n# here we use sapply to calculate the average median intensity\n# for each hugo symbol. this approach is faster than using a for loop.\n# whenever you can i suggest to use sapply in R instead of a for loop.\nmean_intensity &lt;- sapply(\n    duplicated_symbols$ILMN_Gene,\n    function(symbol, gse_ai, fdata){\n        \n        ilmn_ids &lt;- fdata %&gt;% dplyr::filter(\n            ILMN_Gene == symbol\n        ) %&gt;% dplyr::pull(ID)\n        \n        colMeans(exprs(gse_ai)[ilmn_ids, ], na.rm = TRUE)\n        \n    },\n    gse_ai = gse_ai,\n    fdata = fData(gse_ai)\n)\n\nWe now remove from the expression matrix the duplicated genes and then add their mean values.\n\nexprs_vals &lt;- exprs(gse_ai)[-which(rownames(gse_ai) %in% duplicated_ilmns$ID), ]\n\n# first change the name of the current illumina ids \nrownames(exprs_vals) &lt;- fData(gse_ai)[rownames(exprs_vals), \"ILMN_Gene\"]\n\n# now we add the average values\nexprs_vals &lt;- rbind(\n    exprs_vals,\n    t(data.frame(mean_intensity))\n)\n\nAnd before saving the final object, we clean the clinical data available to us so it is easier to work with. Below we show all the columns available to see which columns will be dropped.\n\nfinal_col_data %&gt;% colnames\n\nAfter inspecting, the columns that will be dropped are shown below.\n\ncolumns_to_drop &lt;- c(\n    \"geo_accession\", \"status\", \"submission_date\", \"last_update_date\", \n    \"type\", \"channel_count\", \"source_name_ch1\", \"organism_ch1\",\n    \"molecule_ch1\", \"extract_protocol_ch1\", \"label_ch1\", \"label_protocol_ch1\",\n    \"taxid_ch1\", \"hyb_protocol\", \"scan_protocol\", \"data_processing\",\n    \"platform_id\", \"contact_name\", \"contact_email\", \"contact_laboratory\",\n    \"contact_department\", \"contact_institute\", \"contact_address\", \"contact_city\",\n    \"contact_state\", \"contact_zip/postal_code\", \"contact_country\",\n    \"supplementary_file\", \"data_row_count\", \"relation\",\n    \"Sex:ch1\", \"tissue:ch1\", \"characteristics_ch1\",\n    \"characteristics_ch1.1\", \"characteristics_ch1.2\", \"characteristics_ch1.4\",\n    \"characteristics_ch1.5\", \"her2:ch1\", \"timepoint:ch1\", \"sampling time:ch1\",\n    \"group\", \"paired_or_baseline_single\", # all samples are paired in this case \n    \"number_254_tumours_control_56_ai_treated_198\", \"characteristics_ch1.3\",\n    \"disease:ch1\", \"subtype:ch1\", \"description\", \"title\"\n)\n\nfinal_col_data_drop &lt;- final_col_data %&gt;% \n    dplyr::mutate(pam50 = as.character(`subtype:ch1`)) %&gt;%\n    dplyr::select(-dplyr::one_of(columns_to_drop)) %&gt;% \n    dplyr::mutate(\n        timepoint = stringr::str_replace_all(timepoint, \"_ki67\", \"\"),\n        r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent = ifelse(\n            is.na(r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent),\n            \"not_available\",\n            ifelse(\n                r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent == \"non-responder\",\n                \"non_responder\",\n                r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent\n            )\n        ),\n        ccca_surgery_ki67_2_7 = ifelse(\n            is.na(ccca_surgery_ki67_2_7),\n            \"not_available\",\n            ccca_surgery_ki67_2_7\n        ),\n        pam50 = tolower(pam50)\n    ) %&gt;% \n    dplyr::mutate(\n        pam50 = ifelse(is.na(pam50), \"not_available\", pam50),\n        number = as.character(number),\n        name_patient = paste(`group:ch1`, paste0(\"nb\", number), timepoint, sep = \"_\")\n    ) %&gt;% \n    dplyr::rename(\n        group = `group:ch1`,\n        patient_nb = number\n    ) %&gt;% data.frame %&gt;% \n    `rownames&lt;-`(.$name_patient)\n\nGlimpse is a function to show some entries of all your columns in a nice way.\n\ndplyr::glimpse(final_col_data_drop)\n\nAfter cleaning the clinical data, we can merge with the expression data and save using a summarized experiment object. We just make sure the columns are in the right order first.\n\nmatch_names &lt;- match(colnames(exprs_vals), final_col_data_drop$gsm_name)\ncolnames(exprs_vals) &lt;- final_col_data_drop[\n    match_names,\n    \"name_patient\"\n]\n\nAnd we use an object from the package SummarizedExperiment to store the data. The idea is similar to an ExpressionSet, but the functions to access the data are a bit diffrent. Instead of using exprs, one now uses assay and to get the clinical data one uses colData instead of using pData. There is no feature data in this case.\n\ngse_ai &lt;- SummarizedExperiment::SummarizedExperiment(\n    assays = list(\n        normalized_intensity = exprs_vals\n    ),\n    colData = final_col_data_drop[colnames(exprs_vals), ]\n)\n\nI highly suggest to save this object in an rds object, so you do the cleaning process just once and data is ready to use afterwards.\n\nsaveRDS(gse_ai, \"data/gse_ai.rds\")\n\nAnd to reload the data into R the readRDS function can be used.\n\ngse_ai &lt;- readRDS(\"data/gse_ai.rds\")"
  },
  {
    "objectID": "resources.html#pca",
    "href": "resources.html#pca",
    "title": "5  Further resources",
    "section": "5.1 PCA",
    "text": "5.1 PCA\nTo do PCA on R I highly recommend to use the package PCAtools from Kevin Blighe:\n\nhttps://bioconductor.org/packages/release/bioc/html/PCAtools.html\n\nWith this package one can use metadata to customize the PCA biplots.\nTo do a pairs plot you can either use the function pairsplot from the package PCAtoolsor the function ggpairs from the package GGally."
  },
  {
    "objectID": "resources.html#survival-analysis",
    "href": "resources.html#survival-analysis",
    "title": "5  Further resources",
    "section": "5.2 Survival analysis",
    "text": "5.2 Survival analysis\nA very good starter to survival analysis can be found in this page:\n\nhttp://www.sthda.com/english/wiki/survival-analysis-basics\n\nThey describe the basics of survival analysis and how to use the packages survival and survminer.\nAnd in the next post, they explain the cox proportional hazards model.\n\nhttp://www.sthda.com/english/wiki/cox-proportional-hazards-model\n\nHighly recommend to use this when doing survival analysis, as you get estimates for your comparisons and confidence intervals."
  }
]