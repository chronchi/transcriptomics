[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cleaning transcriptomic datasets",
    "section": "",
    "text": "In this book I will show how several publicly available datasets are filtered and organized in a way that is easy to do transcriptomic analysis with them.\nThe book is divided in several chapters, one for each dataset."
  },
  {
    "objectID": "ai.html",
    "href": "ai.html",
    "title": "1  AI - GSE105777",
    "section": "",
    "text": "First load the packages that will be used along the cleaning process. Always do this at the beginning of your script to make things organized.\n\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\n\nlibrary(readxl)\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "ai.html#introduction",
    "href": "ai.html#introduction",
    "title": "1  AI - GSE105777",
    "section": "1.2 Introduction",
    "text": "1.2 Introduction\nWe will download and process the aromatase inhibitor data available at GEO under accession code: GSE105777. The data is associated to the paper https://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-019-1223-z.\nKi67 levels are also available to these patients in the supplementary data. We will also download the data available there and combine it here."
  },
  {
    "objectID": "ai.html#downloading-data",
    "href": "ai.html#downloading-data",
    "title": "1  AI - GSE105777",
    "section": "1.3 Downloading data",
    "text": "1.3 Downloading data\nTo download the data use the package GEOquery from Bioconductor and specify the accession code. Save also the data on a directory of your choice. In this case I chose a subfolder called data in my current directory.\n\ngse_ai <- GEOquery::getGEO(\n    \"GSE105777\", \n    destdir = \"./data\", \n    GSEMatrix = TRUE\n)[[1]]\n\nThe function exprs from the package Biobase (automatically loaded if you have bioconductor) let us check the expression levels of the expression set downloaded with GEOquery.\n\nexprs(gse_ai)[, 1:5] %>% head\n\nAnd since this is a microarray experiment we need information to map the probe ids to the genes. This information is stored in the feature slot of the expression set above. To retrieve it use the function fData.\n\nfData(gse_ai) %>% head\n\nAnd now load the ki67 levels data from the supplementary material. Remember to specify the folder where your file is located. The table is pivoted for the ki67 levels, such that each patient will have two rows, one for baseline and another for surgery.\n\nki67_levels <- readxl::read_excel(\n    path = \"data/13058_2019_1223_MOESM2_ESM.xlsx\",\n    range = \"TableS4!A3:L257\", \n) %>% janitor::clean_names() %>% \n    tidyr::pivot_longer(\n        cols = c(\"baseline_ki67\", \"surgery_ki67\"), \n        names_to = \"timepoint\",\n        values_to = \"ki67\"\n    ) %>% \n    dplyr::mutate(ki67 = as.numeric(ki67)) %>%\n    dplyr::mutate(\n        title = ifelse(\n            group == \"Peri AI\",\n            paste0(\n                \"AI.\", \n                number_254_tumours_control_56_ai_treated_198,\n                ifelse(timepoint == \"baseline_ki67\", \"B\", \"S\")\n            ),\n            paste0(\n                number_254_tumours_control_56_ai_treated_198,\n                ifelse(timepoint == \"baseline_ki67\", \"B\", \"S\")\n            )\n        )\n    )\n\nki67_levels %>% head"
  },
  {
    "objectID": "ai.html#cleaning-and-integrating",
    "href": "ai.html#cleaning-and-integrating",
    "title": "1  AI - GSE105777",
    "section": "1.4 Cleaning and integrating",
    "text": "1.4 Cleaning and integrating\nWe start now integrating the clinical data available from the GEOquery with the ki67 levels data.\n\n# first convert the title of control samples (reanalysis) to the same\n# format as the treated patients. this will make it easier down the line\n# to match samples\ncol_data <- pData(gse_ai) %>%\n    dplyr::mutate(\n        title = ifelse(\n            !stringr::str_detect(title, \"reanalysis\"), \n            title,\n            paste0(\n                \"Control.\", \n                as.integer(stringr::str_extract(title, \"\\\\d+\")),\n                ifelse(`sampling time:ch1` == \"diagnosis\", \"B\", \"S\")\n            )\n        )\n    )\n\ncol_data$title %>% tail\n\nWe now add both together. The function inner_join from the dplyr package is very powerful. Remember to always specify by which column you want to merge. It is usually very fast to merge tables this way, since it handles the matching of the columns for you.\n\nfinal_col_data <- dplyr::inner_join(\n    ki67_levels,\n    col_data %>% tibble::rownames_to_column(var = \"gsm_name\"),\n    by = \"title\"\n)\n\nTo check the total number of patients, use the code below.\n\nfinal_col_data$patient_id %>% table %>% length\n\nThe number of controls are:\n\nfinal_col_data %>% dplyr::filter(\n    group == \"No Peri AI\"\n) %>% janitor::tabyl(number) %>% nrow\n\nAnd the number of treated patients are:\n\nfinal_col_data %>% dplyr::filter(\n    group == \"Peri AI\"\n) %>% janitor::tabyl(number) %>% nrow\n\nAnd since we have multiple illumina IDs mapping to the same gene, we take their average.\n\n# we first start removing the duplicated genes, but\n# we will change the average expression levels later\ngse_ai <- gse_ai[-which(is.na(exprs(gse_ai)), arr.ind = TRUE)[, 1], ]\n\nBelow we calculate the average intensity for probe ids with multiple hugo symbols.\n\n# first get the duplicated symbols\nduplicated_symbols <- fData(gse_ai) %>%\n    janitor::tabyl(ILMN_Gene) %>%\n    dplyr::filter(n > 1)\n\n# and check what are the probe ids available in the data\nduplicated_ilmns <- fData(gse_ai) %>% dplyr::filter(\n    ILMN_Gene %in% duplicated_symbols$ILMN_Gene\n)\n\n# here we use sapply to calculate the average median intensity\n# for each hugo symbol. this approach is faster than using a for loop.\n# whenever you can i suggest to use sapply in R instead of a for loop.\nmean_intensity <- sapply(\n    duplicated_symbols$ILMN_Gene,\n    function(symbol, gse_ai, fdata){\n        \n        ilmn_ids <- fdata %>% dplyr::filter(\n            ILMN_Gene == symbol\n        ) %>% dplyr::pull(ID)\n        \n        colMeans(exprs(gse_ai)[ilmn_ids, ], na.rm = TRUE)\n        \n    },\n    gse_ai = gse_ai,\n    fdata = fData(gse_ai)\n)\n\nWe now remove from the expression matrix the duplicated genes and then add their mean values.\n\nexprs_vals <- exprs(gse_ai)[-which(rownames(gse_ai) %in% duplicated_ilmns$ID), ]\n\n# first change the name of the current illumina ids \nrownames(exprs_vals) <- fData(gse_ai)[rownames(exprs_vals), \"ILMN_Gene\"]\n\n# now we add the average values\nexprs_vals <- rbind(\n    exprs_vals,\n    t(data.frame(mean_intensity))\n)\n\nAnd before saving the final object, we clean the clinical data available to us so it is easier to work with. Below we show all the columns available to see which columns will be dropped.\n\nfinal_col_data %>% colnames\n\nAfter inspecting, the columns that will be dropped are shown below.\n\ncolumns_to_drop <- c(\n    \"geo_accession\", \"status\", \"submission_date\", \"last_update_date\", \n    \"type\", \"channel_count\", \"source_name_ch1\", \"organism_ch1\",\n    \"molecule_ch1\", \"extract_protocol_ch1\", \"label_ch1\", \"label_protocol_ch1\",\n    \"taxid_ch1\", \"hyb_protocol\", \"scan_protocol\", \"data_processing\",\n    \"platform_id\", \"contact_name\", \"contact_email\", \"contact_laboratory\",\n    \"contact_department\", \"contact_institute\", \"contact_address\", \"contact_city\",\n    \"contact_state\", \"contact_zip/postal_code\", \"contact_country\",\n    \"supplementary_file\", \"data_row_count\", \"relation\",\n    \"Sex:ch1\", \"tissue:ch1\", \"characteristics_ch1\",\n    \"characteristics_ch1.1\", \"characteristics_ch1.2\", \"characteristics_ch1.4\",\n    \"characteristics_ch1.5\", \"her2:ch1\", \"timepoint:ch1\", \"sampling time:ch1\",\n    \"group\", \"paired_or_baseline_single\", # all samples are paired in this case \n    \"number_254_tumours_control_56_ai_treated_198\", \"characteristics_ch1.3\",\n    \"disease:ch1\", \"subtype:ch1\", \"description\", \"title\"\n)\n\nfinal_col_data_drop <- final_col_data %>% \n    dplyr::mutate(pam50 = as.character(`subtype:ch1`)) %>%\n    dplyr::select(-dplyr::one_of(columns_to_drop)) %>% \n    dplyr::mutate(\n        timepoint = stringr::str_replace_all(timepoint, \"_ki67\", \"\"),\n        r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent = ifelse(\n            is.na(r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent),\n            \"not_available\",\n            ifelse(\n                r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent == \"non-responder\",\n                \"non_responder\",\n                r_or_no_r_change_ki67_60_and_baseline_ki67_5_percent\n            )\n        ),\n        ccca_surgery_ki67_2_7 = ifelse(\n            is.na(ccca_surgery_ki67_2_7),\n            \"not_available\",\n            ccca_surgery_ki67_2_7\n        ),\n        pam50 = tolower(pam50)\n    ) %>% \n    dplyr::mutate(\n        pam50 = ifelse(is.na(pam50), \"not_available\", pam50),\n        number = as.character(number),\n        name_patient = paste(`group:ch1`, paste0(\"nb\", number), timepoint, sep = \"_\")\n    ) %>% \n    dplyr::rename(\n        group = `group:ch1`,\n        patient_nb = number\n    ) %>% data.frame %>% \n    `rownames<-`(.$name_patient)\n\nGlimpse is a function to show some entries of all your columns in a nice way.\n\ndplyr::glimpse(final_col_data_drop)\n\nAfter cleaning the clinical data, we can merge with the expression data and save using a summarized experiment object. We just make sure the columns are in the right order first.\n\nmatch_names <- match(colnames(exprs_vals), final_col_data_drop$gsm_name)\ncolnames(exprs_vals) <- final_col_data_drop[\n    match_names,\n    \"name_patient\"\n]\n\nAnd we use an object from the package SummarizedExperiment to store the data. The idea is similar to an ExpressionSet, but the functions to access the data are a bit diffrent. Instead of using exprs, one now uses assay and to get the clinical data one uses colData instead of using pData. There is no feature data in this case.\n\ngse_ai <- SummarizedExperiment::SummarizedExperiment(\n    assays = list(\n        normalized_intensity = exprs_vals\n    ),\n    colData = final_col_data_drop[colnames(exprs_vals), ]\n)\n\nI highly suggest to save this object in an rds object, so you do the cleaning process just once and data is ready to use afterwards.\n\nsaveRDS(gse_ai, \"data/gse_ai.rds\")\n\nAnd to reload the data into R the readRDS function can be used.\n\ngse_ai <- readRDS(\"data/gse_ai.rds\")"
  },
  {
    "objectID": "metabric.html",
    "href": "metabric.html",
    "title": "2  METABRIC",
    "section": "",
    "text": "First load the packages that will be used along the cleaning process. Always do this at the beginning of your script to make things organized.\n\nlibrary(DESeq2)\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nlibrary(dplyr)"
  },
  {
    "objectID": "metabric.html#introduction",
    "href": "metabric.html#introduction",
    "title": "2  METABRIC",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nTo download the clinical data and expression levels from the METABRIC cohort go to cbioportal and select the respective cohort (METABRIC): https://www.cbioportal.org/\nHere we will load and format the metabric data in the same way as SCAN-B and the AI dataset, so it is standardized and better to use in future analysis. For this, we use the summarized experiment object to store expression data and clinical information."
  },
  {
    "objectID": "metabric.html#downloading-data",
    "href": "metabric.html#downloading-data",
    "title": "2  METABRIC",
    "section": "2.3 Downloading data",
    "text": "2.3 Downloading data\nLoading gene expression levels.\n\nexpression_data <- read.csv(\n    \"data_mrna_agilent_microarray.txt\",\n    sep = \"\\t\",\n    check.names = FALSE\n) %>% dplyr::mutate(Entrez_Gene_Id = NULL) %>%\n    `colnames<-`(stringr::str_replace_all(colnames(.), stringr::fixed(\".\"), \"-\"))\n\ndim(expression_data)\n\nWe note that the first two columns correspond to HUGO symbol and ENTREZ ID.\nLoad now clinical data.\n\nclinical_data <- read.csv(\n    \"data_clinical_patient.txt\",\n    sep = \"\\t\",\n    comment.char = \"#\"\n)\n\nglimpse(clinical_data)"
  },
  {
    "objectID": "metabric.html#cleaning-and-integrating",
    "href": "metabric.html#cleaning-and-integrating",
    "title": "2  METABRIC",
    "section": "2.4 Cleaning and integrating",
    "text": "2.4 Cleaning and integrating\nWe see that there are over 2500 rows, meaning that we have more patients with clinical data than expression levels. Let us now select patients that have expression levels.\n\nlength(unique(clinical_data$PATIENT_ID)) == nrow(clinical_data)\n\nWe see that each row has a unique identifier, the patient ID, so we set this as a rowname.\n\nrownames(clinical_data) <- clinical_data$PATIENT_ID\n\nAnd we check if all patients from expression data have clinical data.\n\nlength(\n    intersect(\n        clinical_data$PATIENT_ID, \n        colnames(expression_data[, -c(1,2)])\n    )\n) == ncol(expression_data)-2\n\nIndeed it has, therefore we can just subselect clinical data.\n\nduplicated_genes <- expression_data$Hugo_Symbol[\n    duplicated(expression_data$Hugo_Symbol)\n]\n\nhugo_symbols_duplicated <- expression_data$Hugo_Symbol[\n    duplicated(expression_data$Hugo_Symbol)\n] %>% unique\n\nmedian_genes <- sapply(\n    hugo_symbols_duplicated,\n    function(gene, df){\n        df %>% dplyr::filter(Hugo_Symbol == gene) %>%\n            dplyr::mutate(Hugo_Symbol = NULL) %>%\n            as.matrix(.) %>%\n            MatrixGenerics::colMedians(.)\n    },\n    df = expression_data\n)\n\nmetabric_exp <- expression_data[!duplicated(expression_data$Hugo_Symbol), ]\nrownames(metabric_exp) <- expression_data$Hugo_Symbol[\n    !duplicated(expression_data$Hugo_Symbol)\n]\nmetabric_exp$Hugo_Symbol <- NULL\nmetabric_exp[colnames(median_genes), ] <- median_genes %>% t\n\n\nclinical_data <- clinical_data[\n    intersect(colnames(metabric_exp), rownames(clinical_data)), \n]\n\nBefore we just average the duplicated genes median intensity.\n\n# add new clinical data to the summarized experiment object.\nmetabric <- SummarizedExperiment::SummarizedExperiment(\n    assays = list(\n        median_intensity = metabric_exp[, rownames(clinical_data)]\n    ), \n    colData = clinical_data\n)\n\nAnd we can finally save the RDS file to load it up faster later.\n\n# save the final dataset\nsaveRDS(\n    metabric, \n    file = file.path(\"metabric_filtered.rds\")\n)"
  },
  {
    "objectID": "scanb.html",
    "href": "scanb.html",
    "title": "3  SCANB",
    "section": "",
    "text": "First load the packages that will be used along the cleaning process. Always do this at the beginning of your script to make things organized.\n\nlibrary(DESeq2)\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nlibrary(dplyr)"
  },
  {
    "objectID": "scanb.html#introduction",
    "href": "scanb.html#introduction",
    "title": "3  SCANB",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nSCAN-B is an early stage breast cancer cohort from Sweden. Clinical and gene expression level data is available for several patients.\nTo download the gene expression levels, one can check in the GEO database under the accession code GSE96058. One can download the clinical data in their mutation explorer tool found here: https://oncogenomics.bmc.lu.se/MutationExplorer/"
  },
  {
    "objectID": "scanb.html#downloading-data",
    "href": "scanb.html#downloading-data",
    "title": "3  SCANB",
    "section": "3.3 Downloading data",
    "text": "3.3 Downloading data\nWe downloaded the clinical data from SCAN-B website and the expression matrix (FPKM) from the GEO website using accession code GSE96058.\n\nsweden_clin_data <- read.table(\n    \"data/clinical_data.tsv\",\n    header = TRUE,\n    sep = \"\\t\"\n)\n\nglimpse(sweden_clin_data)\n\nLoading the gene expression levels.\n\ngene_expression_levels <- read.table(\n    paste0(\n        \"data/\",\n        \"GSE96058_gene_expression_3273_samples_and_136_replicates_transformed.csv\"), \n    sep = \",\",\n    header = TRUE,\n    row.names = 1\n)\n\nWe check the dimension of the dataset.\n\ndim(gene_expression_levels)\n\nNow we fetch the clinical data from the GEO database.\n\ngse <- getGEO(\n    GEO = \"GSE96058\", \n    GSEMatrix = TRUE, \n    destdir = \"./data\"\n)\n\npheno_data <- pData(gse[[\"GSE96058-GPL11154_series_matrix.txt.gz\"]])"
  },
  {
    "objectID": "scanb.html#cleaning-and-integrating",
    "href": "scanb.html#cleaning-and-integrating",
    "title": "3  SCANB",
    "section": "3.4 Cleaning and integrating",
    "text": "3.4 Cleaning and integrating\nWe remove the samples with the replicate in the name, as these won’t be necessary for the analysis.\n\ngene_no_replicates <- gene_expression_levels[\n    , \n    !grepl(colnames(gene_expression_levels), pattern = \"repl\")\n]\n\n# select only patients that have clinical information\ngene_no_replicates <- gene_no_replicates[\n    , \n    intersect(colnames(gene_no_replicates), pheno_data$title)\n]\n\nAnd then we need to match the columns from the clinical data with the gene expression levels.\n\n# subselect clinical data and change row names to match column names\n# from expression levels. We will create then a summarized experiment\n# table. \npheno_data_sub <- pheno_data[\n    match(colnames(gene_no_replicates), pheno_data$title), \n]\n\n# check if expression levels number of columns and number of rows have the\n# same patients\nlength(intersect(colnames(gene_no_replicates), pheno_data_sub$title)) ==\n  ncol(gene_no_replicates)\n\nrownames(pheno_data_sub) <- pheno_data_sub$title\n\nAnd now we can merge the downloaded clinical data.\n\n# first we will fetch the pheno data downloaded from the scan-b mutation\n# explorer\nscanb_id <- sapply(\n    pheno_data_sub[, \"scan-b external id:ch1\"], \n    stringr::str_extract,\n    pattern = \"S.*\\\\.g\"\n)\n\npheno_data_sub$scanb_id <- scanb_id\n\n# subselect patients that are in both datasets\nscanb_both <- intersect(scanb_id, sweden_clin_data$SAMPLE)\n\n# subselect now \npheno_data_subsub <- pheno_data_sub %>% \n  dplyr::filter(scanb_id %in% scanb_both)\n\ndim(pheno_data_subsub)\n\n\n# we now subselect the rows from sweden_clin_data using the scanb_id\nsweden_clin_data_sub <- sweden_clin_data %>% \n    dplyr::filter(SAMPLE %in% scanb_both)\n\n# add filename F* as a column name\nname_expression <- sapply(\n    sweden_clin_data_sub$SAMPLE,\n    function(x){\n        pheno_data_subsub[which(x == pheno_data_subsub$scanb_id), \"title\"]\n    }\n)\n\nlength(unique(name_expression)) == nrow(pheno_data_subsub)\n\n\nsweden_clin_data_sub$name_expression <- name_expression\nrownames(sweden_clin_data_sub) <- sweden_clin_data_sub$name_expression\n\nAnd as with the AI dataset, we create a SummarizedExperiment object to save the data and use it later in the downstream analysis.\n\n# add new clinical data to the summarized experiment object.\nsweden_df <- SummarizedExperiment::SummarizedExperiment(\n    assays = list(logFPKM = gene_no_replicates[, rownames(sweden_clin_data_sub)]), \n    colData = sweden_clin_data_sub\n)\n\n# save the final dataset\nsaveRDS(\n    sweden_df, \n    file = file.path(\"data/sweden_df_filtered.rds\")\n)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "4  Further resources",
    "section": "",
    "text": "Here I list some resources you can check to perform your analysis."
  },
  {
    "objectID": "resources.html#pca",
    "href": "resources.html#pca",
    "title": "4  Further resources",
    "section": "4.1 PCA",
    "text": "4.1 PCA\nTo do PCA on R I highly recommend to use the package PCAtools from Kevin Blighe:\n\nhttps://bioconductor.org/packages/release/bioc/html/PCAtools.html\n\nWith this package one can use metadata to customize the PCA biplots.\nTo do a pairs plot you can either use the function pairsplot from the package PCAtoolsor the function ggpairs from the package GGally."
  },
  {
    "objectID": "resources.html#survival-analysis",
    "href": "resources.html#survival-analysis",
    "title": "4  Further resources",
    "section": "4.2 Survival analysis",
    "text": "4.2 Survival analysis\nA very good starter to survival analysis can be found in this page:\n\nhttp://www.sthda.com/english/wiki/survival-analysis-basics\n\nThey describe the basics of survival analysis and how to use the packages survival and survminer.\nAnd in the next post, they explain the cox proportional hazards model.\n\nhttp://www.sthda.com/english/wiki/cox-proportional-hazards-model\n\nHighly recommend to use this when doing survival analysis, as you get estimates for your comparisons and confidence intervals."
  }
]